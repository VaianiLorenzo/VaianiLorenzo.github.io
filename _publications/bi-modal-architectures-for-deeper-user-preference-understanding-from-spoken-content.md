---
title: "Bi-modal architectures for deeper user preference understanding from spoken content"
collection: publications
permalink: /publication/bi-modal-architectures-for-deeper-user-preference-understanding-from-spoken-content
excerpt: 'The analysis of spoken content often relies on a two-stage architecture, which comprises (i) audio-to-text conversion and (ii) transcript processing. This paper elaborates on practical use cases in which audio-related content can be processed jointly with textual data to better capture user preferences.'
date: 2022-02-01
venue: 'AudioCHI 2022 Workshop'
paperurl: 'https://speechretrievalworkshop.github.io/'
citation: 'Moreno La Quatra, Lorenzo Vaiani, Luca Cagliero, and Paolo Garza. 2022. Bi-modal architectures for deeper user preference understanding from spoken content.'
---

Keywords: Multimodal Learning, Speech Processing, Spoken Language Understanding

The analysis of spoken content often relies on a two-stage architecture, which comprises (i) audio-to-text conversion and (ii) transcript processing. However, discarding additional information hidden in audio signals could result in losing the underlying information that can be used for improving text-based analysis and user engagement. Being more correlated with emotional features, audio features could unveil hidden user preferences towards specific multimedia content. This paper elaborates on practical use cases in which audio-related content can be processed jointly with textual data to better capture user preferences.

<!--[Download paper here](http://academicpages.github.io/files/paper1.pdf)-->

You can find the paper here: [https://speechretrievalworkshop.github.io/](https://speechretrievalworkshop.github.io/)
